# üè• **Sistema de Processamento de Dados ANS** 

*Solu√ß√£o completa para coleta, transforma√ß√£o e an√°lise de dados da Ag√™ncia Nacional de Sa√∫de Suplementar*

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue?logo=python)](https://python.org)
[![Django](https://img.shields.io/badge/Django-4.2-brightgreen?logo=django)](https://djangoproject.com)
[![MySQL](https://img.shields.io/badge/MySQL-8.0-orange?logo=mysql)](https://mysql.com)

## üìå **Sum√°rio**
1. [Funcionalidades](#-funcionalidades)
2. [Tecnologias](#-tecnologias)
3. [Estrutura do Projeto](#-estrutura-do-projeto)
4. [Modelagem do Banco](#-modelagem-do-banco)
5. [Como Executar](#-como-executar)
6. [Queries SQL](#%EF%B8%8F-queries-sql-chave)
7. [Exemplos de Sa√≠da](#-exemplos-de-sa√≠da)

## üöÄ **Funcionalidades**

## üìÇ **Teste 1 - Web Scraping de Documentos ANS**

### üéØ **Objetivo**
Automatizar o download dos Anexos I e II (PDFs) do portal da ANS e compact√°-los em um √∫nico arquivo ZIP.

### ‚öôÔ∏è **Tecnologias Utilizadas**
- **Python 3.8+**
- Bibliotecas:
  - `requests` (requisi√ß√µes HTTP)
  - `BeautifulSoup` (parseamento HTML)
  - `zipfile` (compacta√ß√£o)
  - `os` (manipula√ß√£o de arquivos)

### üìú **C√≥digo Principal**
```python
import requests
from bs4 import BeautifulSoup
import zipfile
import os

def baixar_pdfs(url_alvo, arquivo_saida):
    # Configura headers para evitar bloqueio
    headers = {'User-Agent': 'Mozilla/5.0'}
    
    try:
        # Faz requisi√ß√£o ao site
        resposta = requests.get(url_alvo, headers=headers)
        resposta.raise_for_status()
        
        # Parseia o HTML
        soup = BeautifulSoup(resposta.text, 'html.parser')
        
        # Encontra todos os links de PDF
        links_pdf = [
            a['href'] for a in soup.find_all('a', href=True) 
            if a['href'].lower().endswith('.pdf')
        ]
        
        # Cria ZIP com os arquivos
        with zipfile.ZipFile(arquivo_saida, 'w') as zipf:
            for link in links_pdf:
                nome_arquivo = link.split('/')[-1]
                conteudo_pdf = requests.get(link).content
                zipf.writestr(nome_arquivo, conteudo_pdf)
                
        return f"Arquivo {arquivo_saida} criado com sucesso!"
        
    except Exception as e:
        return f"Erro: {str(e)}"
````
## üõ† **Teste 2 - Transforma√ß√£o de Dados**

### üìã **Objetivo**
Extrair dados tabulares do PDF (Anexo I) baixado no Teste 1, transform√°-los em CSV estruturado e compactar o resultado.

### üì• **Entrada**
- Arquivo PDF `Anexo_I.pdf` (baixado via Teste 1)

### üì§ **Sa√≠da**
- Arquivo CSV com dados estruturados
- Arquivo ZIP contendo o CSV (`Teste_[Nome].zip`)

### üß† **L√≥gica Implementada**
```python
def extract_tables_from_pdf(file_path):
    data = []
    with pdfplumber.open(file_path) as pdf:
        for page in pdf.pages:
            tables = page.extract_table()
            if tables:
                for row in tables:
                    clean_row = [cell for cell in row if cell and not cell.isdigit()]
                    data.append(clean_row)
    
    df = pd.DataFrame(data)
    df.rename(columns={"OD": "Seg. Odontol√≥gica", "AMB": "Seg. Ambulatorial"}, inplace=True)
    return df
